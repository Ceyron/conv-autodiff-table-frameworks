<!DOCTYPE html>
<html lang="en">
<head>
<title>Automatic Differentiation Primitive Rules for Convolution in Frameworks</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
</head>
<body class="p-2">
<h1 class="text-center">Convolution Autodiff Primitive Rules in Frameworks</h1>
<p class="text-center">
  Created with ❤️ by <a href="https://www.youtube.com/@MachineLearningSimulation">Machine Learning & Simulation</a>.
</p>
<p class="text-center">
  <a href="https://twitter.com/felix_m_koehler?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @felix_m_koehler</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</p>
<p class="text-center">
  Also check out the pullback/vJp rules in more mathmatical notation <a
  href="http://fkoehler.site/conv-autodiff-table/">fkoehler.site/conv-autodiff-table</a>.
  However, those rules assume a one-channel to one-channel convolution of only
  one samples (i.e., no batched convolutions). Here, we consider the general
  convolution routines of major deep learning frameworks.
</p>
<p class="text-center">
  Important: Take care that same frameworks (e.g., PyTorch) use
  <b>cross-correlation</b> instead of <b>convolution</b>. That changes the
  pullback rule into the filter (see link above).
</p>
<p class="text-center">
  <b>Legend:</b> B = batch size, C_i = input channels, C_o = output channels,
  K = kernel size, N = spatial size
</p>

<table class="table table-striped table-hover text-center">
  <thead>
    <tr>
      <th>Primitive</th>
      <th>Primal</th>
      <th>Pullback/vJp into filter</th>
      <th>Pullback/vJp into input</th>
    </tr>
  <tbody>
  <tr>
    <td><b>Julia NNlib</b></td>
    <td>x = NC_iB</td>
    <td>w = KC_iC_o</td>
    <td>y = NC_oB</td>
  </tr>
  <tr>
    <td>1D "Same"-Padding Convolution</td>
    <td>
      <tt>y = conv(x, w, pad=1)</tt>
    </td>
    <td>
      <tt>dw = conv(permutedims(dy, (1, 3, 2)), permutedims(x, (1, 3, 2)), pad=1, flipped=true)</tt>
    </td>
    <td>
      <tt>dx = conv(dy, permutedims(1, 3, 2), pad=1, flipped=true)</tt>
    </td>
  </tr>
  <tr>
    <td><b>JAX (uses cross-correlation of XLA backend)</b></td>
    <td>x = BC_iN</td>
    <td>w = C_iC_oK</td>
    <td>y = BC_oN</td>
  </tr>
  <tr>
    <td>1D "Same"-Padding Convolution</td>
    <td>
      <tt>y = lax.conv_general_dilated(x, w, (1,), ((1, 1),))</tt>
    </td>
    <td>
      <tt>dw = jnp.flip(lax.conv_general_dilated(jnp.transpose(dy, (1, 0, 2)), jnp.transpose(x, (1, 0, 2)), (1,), ((1, 1),)), 2) </tt>
    </td>
    <td>
      <tt>dx = lax.conv_general_dilated(dy, jnp.transpose(jnp.flip(w, 2), (1, 0, 2)), (1,), ((1, 1),))</tt>
    </td>
  </tr>
  </tbody>
</table> 


<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js" integrity="sha384-w76AqPfDkMBDXo30jS1Sgez6pr3x5MlQ1ZAGC+nuZB+EYdgRZgiwxhTBTkF7CXvN" crossorigin="anonymous"></script>
</body>
</html>
